import os
import json
import argparse
from loguru import logger
import pandas as pd
import traceback
from .score_evaluator import ScoreEvaluator
from .compile_execute import CompileExecute

def setup_logger():
    """Configure logger settings"""
    os.makedirs("logs", exist_ok=True)
    logger.remove()
    logger.add(
        "logs/evaluation_{time}.log",
        rotation="100 MB",
        level="INFO",
        format="{time} {level} {message}"
    )
    logger.add(lambda msg: print(msg), level="INFO")

def parse_arguments():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(description="Evaluate COBOL code generated by LLMs")
    parser.add_argument(
        "--model", 
        type=str, 
        default=None,
        help="Model used for generation"
    )
    parser.add_argument(
        "--mode", 
        type=str, 
        default=None,
        choices=["Complete", "Instruct"],
        help="Generation mode that was used"
    )
    parser.add_argument(
        "--csv", 
        type=str, 
        default=None,
        help="Path to CSV file with generation results"
    )
    parser.add_argument(
        "--bert-score", 
        action="store_true",
        help="Calculate BERT score"
    )
    parser.add_argument(
        "--compile-execute", 
        action="store_true",
        help="Compile and execute the generated code"
    )
    return parser.parse_args()

def run_bert_evaluation(model_name, csv_path):
    """Run BERT score evaluation on generated results"""
    try:
        if not os.path.exists(csv_path):
            logger.error(f"File not found: {csv_path}")
            return False

        df = pd.read_csv(csv_path)
        logger.info(f"Loaded {len(df)} records from {csv_path}")

        golden_set = []
        for _, row in df.iterrows():
            try:
                golden_set.append({
                    "query": row.get('Cobol_Eval', ''),
                    "expected_response": row.get('Expected_Program', '')
                })
            except KeyError as e:
                logger.warning(f"Missing key in row: {e}")

        # Make sure column names match what's expected in evaluate()
        instruction_set = df.rename(columns={
            'program_name': 'Program_name',
            'query': 'Cobol_Eval',
            'generated_response': 'Generated_program',
            'expected_response': 'Expected_Program'
        })

        logger.info("Starting BERT score evaluation...")
        scorer = ScoreEvaluator()
        results = scorer.evaluate(golden_set, instruction_set, model_name)
        logger.success("BERT score evaluation completed successfully")
        return results
    
    except Exception as e:
        logger.error(f"Fatal error during BERT score evaluation: {e}")
        logger.error(traceback.format_exc())
        return False

def run_compile_evaluation(model_name, mode, csv_path):
    """Run compilation and execution evaluation"""
    try:
        from src.generator import Model
        model = Model(name=model_name)
        
        logger.info(f"Starting compilation and execution evaluation for {model_name}...")
        evaluator = CompileExecute(model, csv_path, mode)
        results = evaluator.compile()
        
        logger.success("Compilation and execution evaluation completed")
        return results
    except Exception as e:
        logger.error(f"Fatal error during compilation evaluation: {e}")
        logger.error(traceback.format_exc())
        return False

def main():
    setup_logger()
    args = parse_arguments()
    
    # If no specific model/mode provided, try to load from last run
    if args.model is None or args.mode is None:
        try:
            with open("config/last_run.json", "r") as f:
                config = json.load(f)
                model_name = args.model or config.get("model_name")
                mode = args.mode or config.get("mode")
        except:
            logger.error("No model/mode specified and couldn't load from config/last_run.json")
            logger.info("Please specify --model and --mode parameters")
            return
    else:
        model_name = args.model
        mode = args.mode
    
    # Determine CSV path if not provided
    csv_path = args.csv or f"preds/{model_name}_generated_results.csv"
    
    if not os.path.exists(csv_path):
        logger.error(f"CSV file not found: {csv_path}")
        return
    
    # Run evaluations based on arguments
    if args.bert_score:
        logger.info("Running BERT score evaluation...")
        run_bert_evaluation(model_name, csv_path)
    
    if args.compile_execute:
        logger.info("Running compilation and execution evaluation...")
        run_compile_evaluation(model_name, mode, csv_path)
    
    # If no specific evaluation requested, run both
    if not args.bert_score and not args.compile_execute:
        logger.info("Running all evaluations...")
        
        # BERT score evaluation
        bert_results = run_bert_evaluation(model_name, csv_path)
        
        # Compilation and execution evaluation
        compile_results = run_compile_evaluation(model_name, mode, csv_path)
        
        logger.success("All evaluations completed")

if __name__ == "__main__":
    main()